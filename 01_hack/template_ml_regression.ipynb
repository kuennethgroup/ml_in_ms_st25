{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression template\n",
    "\n",
    "This follows https://auto.gluon.ai/stable/tutorials/tabular/tabular-quick-start.html\n",
    "\n",
    "Work through the notebook cells and change to make it work for your project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas numpy autogluon ipywidgets git+https://github.com/Ramprasad-Group/psmiles.git\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from psmiles import PolymerSmiles as PS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from autogluon.tabular import TabularPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data and fingerprint\n",
    "\n",
    "- Create another notebook \"data.ipynb\". Synthesize, modify, manipulate, and save your data as pandas dataframe in this notebook. Finally, save your data using `pd.to_csv('data.csv)',\n",
    "- Replace the following code and load your own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_init = pd.read_json(\n",
    "    \"https://raw.githubusercontent.com/kuennethgroup/materials_datasets/refs/heads/main/polymer_tendency_to_crystalize/polymers_tend_to_crystalize.json\"\n",
    ")[[\"smiles\", \"value\"]]\n",
    "\n",
    "# Compute the fingerprints using the PSMILES package\n",
    "fps = np.vstack(df_init.smiles.apply(lambda x: PS(x).fingerprint()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_fps = MinMaxScaler()\n",
    "fps_scaled = scaler_fps.fit_transform(fps)\n",
    "fps_scaled = pd.DataFrame(fps_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare final data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>11</th>\n",
       "      <th>...</th>\n",
       "      <th>2035</th>\n",
       "      <th>2036</th>\n",
       "      <th>2038</th>\n",
       "      <th>2039</th>\n",
       "      <th>2041</th>\n",
       "      <th>2043</th>\n",
       "      <th>2044</th>\n",
       "      <th>2045</th>\n",
       "      <th>2047</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.483077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.449331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.343636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.201459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.217977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.321342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.257904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.293068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.218991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.075193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>432 rows × 1369 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1    2    4    5    6    7    8    9   11  ...  2035  2036  \\\n",
       "0    0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "1    0.0  0.166667  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "2    1.0  0.166667  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "3    0.0  0.166667  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "4    0.0  0.333333  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "..   ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   ...   \n",
       "427  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "428  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "429  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "430  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "431  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "\n",
       "     2038  2039  2041  2043  2044  2045  2047     value  \n",
       "0     0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.483077  \n",
       "1     0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.449331  \n",
       "2     0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.343636  \n",
       "3     0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.201459  \n",
       "4     0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.217977  \n",
       "..    ...   ...   ...   ...   ...   ...   ...       ...  \n",
       "427   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.321342  \n",
       "428   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.257904  \n",
       "429   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.293068  \n",
       "430   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.218991  \n",
       "431   0.0   0.0   0.0   0.0   0.0   0.0   0.5  0.075193  \n",
       "\n",
       "[432 rows x 1369 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concat fingerprints\n",
    "df = pd.concat((fps_scaled, df_init), axis=1)\n",
    "\n",
    "# drop smiles column because it should not be used for training\n",
    "df = df.drop(columns=\"smiles\")\n",
    "\n",
    "# Make sure they're all float\n",
    "df = df.astype(np.float32)\n",
    "\n",
    "\n",
    "# Remove columns that are zero, if any\n",
    "df = df.loc[:, (df != 0).any(axis=0)]\n",
    "\n",
    "# Normalize the tendency to crystalize\n",
    "# df['value'] = df['value'] / 100\n",
    "\n",
    "scaler_value = MinMaxScaler()\n",
    "df[\"value\"] = scaler_value.fit_transform(df[[\"value\"]])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split in train and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>11</th>\n",
       "      <th>...</th>\n",
       "      <th>2035</th>\n",
       "      <th>2036</th>\n",
       "      <th>2038</th>\n",
       "      <th>2039</th>\n",
       "      <th>2041</th>\n",
       "      <th>2043</th>\n",
       "      <th>2044</th>\n",
       "      <th>2045</th>\n",
       "      <th>2047</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.582590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.286178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.217673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.809282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.300365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.212606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.088873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.320488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>345 rows × 1369 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1    2    4    5    6    7    8    9   11  ...  2035  2036  \\\n",
       "132  0.0  0.166667  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "231  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "31   0.0  0.166667  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "84   0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "296  0.0  0.166667  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "..   ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   ...   \n",
       "71   0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "106  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.5   \n",
       "270  0.0  0.166667  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "348  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "102  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.5   \n",
       "\n",
       "     2038  2039  2041  2043  2044  2045  2047     value  \n",
       "132   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.582590  \n",
       "231   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.286178  \n",
       "31    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.217673  \n",
       "84    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.809282  \n",
       "296   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.002432  \n",
       "..    ...   ...   ...   ...   ...   ...   ...       ...  \n",
       "71    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.300365  \n",
       "106   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.212606  \n",
       "270   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.088873  \n",
       "348   0.0   0.0   1.0   0.0   0.0   0.0   0.0  0.125253  \n",
       "102   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.320488  \n",
       "\n",
       "[345 rows x 1369 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>11</th>\n",
       "      <th>...</th>\n",
       "      <th>2035</th>\n",
       "      <th>2036</th>\n",
       "      <th>2038</th>\n",
       "      <th>2039</th>\n",
       "      <th>2041</th>\n",
       "      <th>2043</th>\n",
       "      <th>2044</th>\n",
       "      <th>2045</th>\n",
       "      <th>2047</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.733685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.536279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.168221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.385083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.635488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.894406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.126752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.373835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.231557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87 rows × 1369 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1    2    4    5    6    7    8     9   11  ...  2035  2036  \\\n",
       "424  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.00  0.0  ...   0.0   0.0   \n",
       "75   0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.25  0.0  ...   0.0   0.0   \n",
       "180  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.5  0.00  0.0  ...   0.0   0.0   \n",
       "30   0.0  0.166667  0.0  0.0  0.0  0.0  0.0  0.0  0.00  0.0  ...   0.0   0.0   \n",
       "392  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.00  0.5  ...   0.0   0.0   \n",
       "..   ...       ...  ...  ...  ...  ...  ...  ...   ...  ...  ...   ...   ...   \n",
       "57   0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.00  0.0  ...   0.0   0.0   \n",
       "124  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.00  0.0  ...   0.0   0.0   \n",
       "24   0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.00  0.0  ...   0.0   0.0   \n",
       "17   0.0  0.166667  0.0  0.0  0.0  0.0  0.0  0.0  0.00  0.0  ...   0.0   0.0   \n",
       "66   0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.00  0.0  ...   0.0   0.0   \n",
       "\n",
       "     2038  2039  2041  2043  2044  2045  2047     value  \n",
       "424   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.083604  \n",
       "75    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.733685  \n",
       "180   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.536279  \n",
       "30    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.168221  \n",
       "392   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.385083  \n",
       "..    ...   ...   ...   ...   ...   ...   ...       ...  \n",
       "57    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.635488  \n",
       "124   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.894406  \n",
       "24    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.126752  \n",
       "17    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.373835  \n",
       "66    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.231557  \n",
       "\n",
       "[87 rows x 1369 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.20, random_state=42)\n",
    "\n",
    "display(df_train)\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train you AutoGluon ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20250624_143411\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.12.8\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024\n",
      "CPU Count:          192\n",
      "Memory Avail:       963.23 GB / 1007.45 GB (95.6%)\n",
      "Disk Space Avail:   1574.14 GB / 7096.34 GB (22.2%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 15s of the 60s of remaining time (25%).\n",
      "\t\tContext path: \"/home/chris/courses/ml-in-ms-st25/01_hack/AutogluonModels/ag-20250624_143411/ds_sub_fit/sub_fit_ho\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.12.8\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024\n",
      "CPU Count:          192\n",
      "Memory Avail:       963.23 GB / 1007.45 GB (95.6%)\n",
      "Disk Space Avail:   1574.14 GB / 7096.34 GB (22.2%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 15s of the 60s of remaining time (25%).\n",
      "\t\tContext path: \"/home/chris/courses/ml-in-ms-st25/01_hack/AutogluonModels/ag-20250624_143411/ds_sub_fit/sub_fit_ho\"\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                    model  score_holdout  score_val              eval_metric  pred_time_test  pred_time_val  fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0         LightGBM_BAG_L1      -0.143172  -0.193740  root_mean_squared_error        0.187397       0.034873  1.065231                 0.187397                0.034873           1.065231            1       True          4\n",
      "1       LightGBMXT_BAG_L2      -0.144130  -0.192652  root_mean_squared_error        1.424583       0.652040  4.476497                 0.184886                0.034293           1.552772            2       True          8\n",
      "2       LightGBMXT_BAG_L1      -0.144551  -0.189068  root_mean_squared_error        0.323291       0.032872  1.042662                 0.323291                0.032872           1.042662            1       True          3\n",
      "3     WeightedEnsemble_L2      -0.148491  -0.187483  root_mean_squared_error        0.681976       0.398504  1.926362                 0.001609                0.000275           0.005881            2       True          7\n",
      "4     WeightedEnsemble_L3      -0.148491  -0.187483  root_mean_squared_error        0.682191       0.398518  1.926255                 0.001824                0.000289           0.005775            3       True          9\n",
      "5         CatBoost_BAG_L1      -0.151475  -0.202383  root_mean_squared_error        0.559331       0.219518  1.003244                 0.559331                0.219518           1.003244            1       True          6\n",
      "6  RandomForestMSE_BAG_L1      -0.165186  -0.197151  root_mean_squared_error        0.057882       0.098457  0.843946                 0.057882                0.098457           0.843946            1       True          5\n",
      "7   KNeighborsUnif_BAG_L1      -0.179982  -0.215233  root_mean_squared_error        0.253191       0.325415  0.033371                 0.253191                0.325415           0.033371            1       True          1\n",
      "8   KNeighborsDist_BAG_L1      -0.183919  -0.210714  root_mean_squared_error        0.299195       0.266900  0.033873                 0.299195                0.266900           0.033873            1       True          2\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t19s\t = DyStack   runtime |\t41s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 41s\n",
      "AutoGluon will save models to \"/home/chris/courses/ml-in-ms-st25/01_hack/AutogluonModels/ag-20250624_143411\"\n",
      "Train Data Rows:    345\n",
      "Train Data Columns: 1368\n",
      "Label Column:       value\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    985546.64 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1.80 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 843 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 100): ['6', '35', '88', '119', '136', '155', '179', '181', '207', '332', '365', '370', '384', '395', '412', '414', '421', '470', '496', '500', '517', '529', '531', '550', '610', '624', '631', '712', '731', '737', '747', '752', '765', '768', '771', '778', '786', '800', '808', '813', '815', '821', '849', '874', '876', '885', '927', '943', '962', '970', '990', '1001', '1005', '1041', '1097', '1124', '1130', '1131', '1148', '1169', '1186', '1190', '1207', '1247', '1264', '1268', '1284', '1295', '1298', '1307', '1373', '1405', '1410', '1418', '1438', '1448', '1468', '1478', '1483', '1528', '1537', '1561', '1613', '1644', '1698', '1704', '1709', '1724', '1732', '1744', '1756', '1761', '1801', '1861', '1880', '1892', '1972', '2014', '2038', '2044']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 387): ['47', '61', '67', '75', '146', '157', '163', '185', '199', '201', '204', '208', '234', '246', '254', '255', '257', '260', '262', '265', '271', '285', '304', '305', '306', '315', '317', '318', '320', '324', '341', '353', '364', '372', '377', '381', '406', '428', '445', '451', '472', '485', '490', '498', '499', '502', '509', '512', '516', '525', '527', '537', '538', '549', '558', '561', '563', '567', '568', '582', '585', '599', '600', '603', '629', '633', '638', '643', '644', '648', '649', '653', '654', '655', '657', '658', '659', '663', '666', '675', '681', '701', '707', '723', '726', '729', '741', '750', '764', '767', '772', '774', '776', '779', '796', '797', '798', '802', '810', '811', '812', '822', '825', '828', '838', '839', '845', '853', '860', '863', '871', '877', '880', '895', '897', '904', '906', '914', '916', '925', '936', '941', '946', '960', '966', '968', '979', '984', '986', '993', '1004', '1020', '1027', '1032', '1042', '1048', '1049', '1050', '1051', '1060', '1061', '1062', '1067', '1068', '1069', '1080', '1082', '1084', '1085', '1089', '1091', '1095', '1098', '1100', '1108', '1109', '1111', '1118', '1123', '1125', '1126', '1127', '1140', '1142', '1155', '1165', '1167', '1168', '1172', '1173', '1175', '1184', '1187', '1189', '1193', '1195', '1197', '1203', '1208', '1210', '1215', '1219', '1222', '1223', '1225', '1228', '1235', '1239', '1245', '1248', '1266', '1271', '1278', '1289', '1296', '1306', '1310', '1322', '1324', '1330', '1331', '1332', '1333', '1335', '1347', '1348', '1353', '1360', '1362', '1363', '1372', '1374', '1378', '1379', '1383', '1393', '1400', '1409', '1411', '1413', '1416', '1421', '1424', '1439', '1445', '1455', '1457', '1463', '1464', '1467', '1470', '1472', '1479', '1481', '1487', '1489', '1490', '1492', '1503', '1506', '1508', '1512', '1514', '1515', '1516', '1517', '1521', '1523', '1527', '1530', '1531', '1534', '1540', '1542', '1545', '1553', '1557', '1559', '1569', '1573', '1576', '1580', '1585', '1597', '1604', '1605', '1606', '1607', '1608', '1609', '1611', '1614', '1618', '1619', '1624', '1625', '1627', '1630', '1633', '1634', '1639', '1646', '1650', '1653', '1654', '1656', '1659', '1672', '1674', '1685', '1692', '1708', '1710', '1712', '1713', '1720', '1721', '1725', '1726', '1733', '1735', '1738', '1741', '1745', '1746', '1751', '1760', '1762', '1764', '1773', '1778', '1779', '1781', '1782', '1786', '1791', '1793', '1796', '1803', '1805', '1807', '1812', '1814', '1819', '1823', '1827', '1829', '1834', '1844', '1845', '1848', '1850', '1858', '1860', '1862', '1863', '1865', '1867', '1868', '1870', '1877', '1883', '1894', '1898', '1899', '1900', '1901', '1904', '1905', '1908', '1912', '1913', '1919', '1922', '1923', '1925', '1929', '1932', '1935', '1937', '1939', '1941', '1950', '1955', '1960', '1961', '1965', '1968', '1971', '1975', '1983', '1993', '1997', '2001', '2003', '2006', '2008', '2013', '2021', '2022', '2026', '2031', '2032', '2034', '2039', '2043', '2045']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 387 | ['47', '61', '67', '75', '146', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 881 | ['0', '1', '2', '4', '5', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 408 | ['1', '2', '8', '9', '11', ...]\n",
      "\t\t('int', ['bool']) : 473 | ['0', '4', '5', '7', '12', ...]\n",
      "\t2.4s = Fit runtime\n",
      "\t881 features in original data used to generate 881 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.69 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 2.5s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 108 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 25.73s of the 38.60s of remaining time.\n",
      "\t-0.2062\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 25.51s of the 38.37s of remaining time.\n",
      "\t-0.2038\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 25.26s of the 38.13s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.02%)\n",
      "\t-0.1838\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.96s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 22.73s of the 35.60s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.02%)\n",
      "\t-0.1888\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.94s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 20.26s of the 33.13s of remaining time.\n",
      "\t-0.1929\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.8s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 19.24s of the 32.11s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.03%)\n",
      "\t-0.1848\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.42s\t = Training   runtime\n",
      "\t0.24s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 10.24s of the 23.10s of remaining time.\n",
      "\t-0.1947\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.58s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 9.42s of the 22.29s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\t-0.198\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.96s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 3.87s of the 16.74s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.03%)\n",
      "\t-0.1883\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.5s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 0.84s of the 13.71s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tTime limit exceeded... Skipping NeuralNetTorch_BAG_L1.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 38.61s of the 8.48s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.467, 'CatBoost_BAG_L1': 0.267, 'XGBoost_BAG_L1': 0.2, 'NeuralNetFastAI_BAG_L1': 0.067}\n",
      "\t-0.1791\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 8.46s of the 8.44s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.02%)\n",
      "\t-0.1825\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.78s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 6.14s of the 6.12s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.02%)\n",
      "\t-0.1841\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.8s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 3.81s of the 3.79s of remaining time.\n",
      "2025-06-24 16:35:07,866\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=1744030, ip=132.180.187.66)\n",
      "  File \"/home/chris/courses/ml-in-ms-st25/.venv/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/chris/courses/ml-in-ms-st25/.venv/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/chris/courses/ml-in-ms-st25/.venv/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 224, in _fit\n",
      "    raise TimeLimitExceeded\n",
      "autogluon.core.utils.exceptions.TimeLimitExceeded\n",
      "2025-06-24 16:35:07,880\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=1744036, ip=132.180.187.66)\n",
      "  File \"/home/chris/courses/ml-in-ms-st25/.venv/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/chris/courses/ml-in-ms-st25/.venv/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/chris/courses/ml-in-ms-st25/.venv/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 224, in _fit\n",
      "    raise TimeLimitExceeded\n",
      "autogluon.core.utils.exceptions.TimeLimitExceeded\n",
      "2025-06-24 16:35:07,880\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=1744033, ip=132.180.187.66)\n",
      "  File \"/home/chris/courses/ml-in-ms-st25/.venv/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/chris/courses/ml-in-ms-st25/.venv/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/chris/courses/ml-in-ms-st25/.venv/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 224, in _fit\n",
      "    raise TimeLimitExceeded\n",
      "autogluon.core.utils.exceptions.TimeLimitExceeded\n",
      "2025-06-24 16:35:07,881\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=1744028, ip=132.180.187.66)\n",
      "  File \"/home/chris/courses/ml-in-ms-st25/.venv/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/chris/courses/ml-in-ms-st25/.venv/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/chris/courses/ml-in-ms-st25/.venv/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 224, in _fit\n",
      "    raise TimeLimitExceeded\n",
      "autogluon.core.utils.exceptions.TimeLimitExceeded\n",
      "2025-06-24 16:35:07,882\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=1744032, ip=132.180.187.66)\n",
      "  File \"/home/chris/courses/ml-in-ms-st25/.venv/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/chris/courses/ml-in-ms-st25/.venv/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/chris/courses/ml-in-ms-st25/.venv/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 224, in _fit\n",
      "    raise TimeLimitExceeded\n",
      "autogluon.core.utils.exceptions.TimeLimitExceeded\n",
      "2025-06-24 16:35:07,893\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=1744034, ip=132.180.187.66)\n",
      "  File \"/home/chris/courses/ml-in-ms-st25/.venv/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/chris/courses/ml-in-ms-st25/.venv/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/chris/courses/ml-in-ms-st25/.venv/lib/python3.12/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 224, in _fit\n",
      "    raise TimeLimitExceeded\n",
      "autogluon.core.utils.exceptions.TimeLimitExceeded\n",
      "\t-0.1854\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.63s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 2.93s of the 2.91s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.03%)\n",
      "2025-06-24 16:35:08,860\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\t-0.1789\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.28s\t = Training   runtime\n",
      "\t0.26s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 38.61s of the -1.04s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L2': 0.72, 'RandomForestMSE_BAG_L2': 0.12, 'XGBoost_BAG_L1': 0.08, 'KNeighborsDist_BAG_L1': 0.04, 'NeuralNetFastAI_BAG_L1': 0.04}\n",
      "\t-0.1787\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 42.16s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 61.5 rows/s (44 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/chris/courses/ml-in-ms-st25/01_hack/AutogluonModels/ag-20250624_143411\")\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(\n",
    "    label=\"value\",\n",
    "    problem_type=\"regression\",\n",
    ").fit(df_train, time_limit=60, presets=\"best_quality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use matplotlib for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/courses/ml-in-ms-st25/.venv/lib/python3.12/site-packages/fastai/learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "  warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "/home/chris/courses/ml-in-ms-st25/.venv/lib/python3.12/site-packages/fastai/learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "  warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "/home/chris/courses/ml-in-ms-st25/.venv/lib/python3.12/site-packages/fastai/learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "  warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "/home/chris/courses/ml-in-ms-st25/.venv/lib/python3.12/site-packages/fastai/learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "  warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "/home/chris/courses/ml-in-ms-st25/.venv/lib/python3.12/site-packages/fastai/learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "  warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "/home/chris/courses/ml-in-ms-st25/.venv/lib/python3.12/site-packages/fastai/learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "  warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "/home/chris/courses/ml-in-ms-st25/.venv/lib/python3.12/site-packages/fastai/learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "  warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "/home/chris/courses/ml-in-ms-st25/.venv/lib/python3.12/site-packages/fastai/learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "  warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.452353835105896\n",
      "RMSE: 0.1850890815258026\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, root_mean_squared_error\n",
    "\n",
    "y_pred = predictor.predict(df_test.drop(columns=[\"value\"]))\n",
    "\n",
    "r2 = r2_score(df_test[\"value\"], y_pred)\n",
    "rmse = root_mean_squared_error(df_test[\"value\"], y_pred)\n",
    "\n",
    "print(f\"R2: {r2}\")\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/courses/ml-in-ms-st25/.venv/lib/python3.12/site-packages/fastai/learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "  warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "/home/chris/courses/ml-in-ms-st25/.venv/lib/python3.12/site-packages/fastai/learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "  warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "/home/chris/courses/ml-in-ms-st25/.venv/lib/python3.12/site-packages/fastai/learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "  warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "/home/chris/courses/ml-in-ms-st25/.venv/lib/python3.12/site-packages/fastai/learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "  warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "/home/chris/courses/ml-in-ms-st25/.venv/lib/python3.12/site-packages/fastai/learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "  warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "/home/chris/courses/ml-in-ms-st25/.venv/lib/python3.12/site-packages/fastai/learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "  warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "/home/chris/courses/ml-in-ms-st25/.venv/lib/python3.12/site-packages/fastai/learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "  warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "/home/chris/courses/ml-in-ms-st25/.venv/lib/python3.12/site-packages/fastai/learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "  warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'pred')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "y_pred = predictor.predict(df_test.drop(columns=[\"value\"]))\n",
    "y_pred\n",
    "\n",
    "ax.plot(y_pred, df_test[\"value\"], \"o\")\n",
    "ax.plot([0, 1], [0, 1], \"k--\")\n",
    "ax.text(0.1, 0.9, f\"R2 = {r2:.3f}\", transform=ax.transAxes)\n",
    "ax.text(0.1, 0.85, f\"RMSE = {rmse:.3f}\", transform=ax.transAxes)\n",
    "ax.set_title(\"Testdata set\")\n",
    "ax.set_ylabel(\"truth\")\n",
    "ax.set_xlabel(\"pred\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
